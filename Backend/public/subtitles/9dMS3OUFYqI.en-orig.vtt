WEBVTT
Kind: captions
Language: en

00:00:00.160 --> 00:00:02.389 align:start position:0%
 
I'm<00:00:00.400><c> here</c><00:00:00.480><c> at</c><00:00:00.719><c> CubeCon</c><00:00:01.439><c> and</c><00:00:01.600><c> we're</c><00:00:01.839><c> excited</c><00:00:02.240><c> to</c>

00:00:02.389 --> 00:00:02.399 align:start position:0%
I'm here at CubeCon and we're excited to
 

00:00:02.399 --> 00:00:03.909 align:start position:0%
I'm here at CubeCon and we're excited to
tell<00:00:02.560><c> you</c><00:00:02.720><c> more</c><00:00:02.879><c> about</c><00:00:03.040><c> our</c><00:00:03.200><c> Google</c><00:00:03.520><c> Cloud</c><00:00:03.760><c> and</c>

00:00:03.909 --> 00:00:03.919 align:start position:0%
tell you more about our Google Cloud and
 

00:00:03.919 --> 00:00:05.990 align:start position:0%
tell you more about our Google Cloud and
NVIDIA<00:00:04.400><c> developer</c><00:00:04.799><c> community</c><00:00:05.520><c> and</c><00:00:05.680><c> I've</c><00:00:05.839><c> got</c>

00:00:05.990 --> 00:00:06.000 align:start position:0%
NVIDIA developer community and I've got
 

00:00:06.000 --> 00:00:07.829 align:start position:0%
NVIDIA developer community and I've got
Sanjay<00:00:06.560><c> here</c><00:00:06.720><c> to</c><00:00:06.879><c> tell</c><00:00:07.040><c> me</c><00:00:07.200><c> a</c><00:00:07.359><c> little</c><00:00:07.440><c> bit</c><00:00:07.600><c> more</c>

00:00:07.829 --> 00:00:07.839 align:start position:0%
Sanjay here to tell me a little bit more
 

00:00:07.839 --> 00:00:10.549 align:start position:0%
Sanjay here to tell me a little bit more
about<00:00:08.160><c> harnessing</c><00:00:08.800><c> GPUs</c><00:00:09.519><c> on</c><00:00:09.760><c> Google</c><00:00:10.000><c> Cloud.</c>

00:00:10.549 --> 00:00:10.559 align:start position:0%
about harnessing GPUs on Google Cloud.
 

00:00:10.559 --> 00:00:12.150 align:start position:0%
about harnessing GPUs on Google Cloud.
Could<00:00:10.719><c> you</c><00:00:10.880><c> tell</c><00:00:11.040><c> me</c><00:00:11.200><c> about</c><00:00:11.599><c> making</c><00:00:11.840><c> the</c><00:00:12.080><c> most</c>

00:00:12.150 --> 00:00:12.160 align:start position:0%
Could you tell me about making the most
 

00:00:12.160 --> 00:00:14.070 align:start position:0%
Could you tell me about making the most
of<00:00:12.320><c> AI</c><00:00:12.639><c> deployment</c><00:00:13.040><c> on</c><00:00:13.280><c> Google</c><00:00:13.599><c> Cloud?</c>

00:00:14.070 --> 00:00:14.080 align:start position:0%
of AI deployment on Google Cloud?
 

00:00:14.080 --> 00:00:16.230 align:start position:0%
of AI deployment on Google Cloud?
&gt;&gt; Today<00:00:14.320><c> you</c><00:00:14.559><c> see</c><00:00:14.719><c> that</c><00:00:14.960><c> AI</c><00:00:15.280><c> inference</c><00:00:15.920><c> is</c>

00:00:16.230 --> 00:00:16.240 align:start position:0%
&gt;&gt; Today you see that AI inference is
 

00:00:16.240 --> 00:00:18.070 align:start position:0%
&gt;&gt; Today you see that AI inference is
getting<00:00:16.480><c> deployed</c><00:00:16.960><c> on</c><00:00:17.279><c> single</c><00:00:17.600><c> node</c><00:00:17.840><c> and</c>

00:00:18.070 --> 00:00:18.080 align:start position:0%
getting deployed on single node and
 

00:00:18.080 --> 00:00:19.670 align:start position:0%
getting deployed on single node and
multiple<00:00:18.400><c> nodes</c><00:00:18.960><c> and</c><00:00:19.199><c> it's</c><00:00:19.439><c> getting</c><00:00:19.650><c> [music]</c>

00:00:19.670 --> 00:00:19.680 align:start position:0%
multiple nodes and it's getting [music]
 

00:00:19.680 --> 00:00:21.189 align:start position:0%
multiple nodes and it's getting [music]
deployed<00:00:20.080><c> both</c><00:00:20.320><c> as</c><00:00:20.560><c> aggregated</c><00:00:21.039><c> and</c>

00:00:21.189 --> 00:00:21.199 align:start position:0%
deployed both as aggregated and
 

00:00:21.199 --> 00:00:23.509 align:start position:0%
deployed both as aggregated and
disagregated<00:00:21.840><c> systems.</c><00:00:22.640><c> So</c><00:00:22.880><c> it's</c><00:00:23.199><c> getting</c>

00:00:23.509 --> 00:00:23.519 align:start position:0%
disagregated systems. So it's getting
 

00:00:23.519 --> 00:00:25.349 align:start position:0%
disagregated systems. So it's getting
more<00:00:23.920><c> difficult</c><00:00:24.320><c> to</c><00:00:24.560><c> manage</c><00:00:24.800><c> all</c><00:00:25.039><c> of</c><00:00:25.119><c> these</c>

00:00:25.349 --> 00:00:25.359 align:start position:0%
more difficult to manage all of these
 

00:00:25.359 --> 00:00:27.509 align:start position:0%
more difficult to manage all of these
components<00:00:26.000><c> well</c><00:00:26.320><c> with</c><00:00:26.640><c> Kubernetes</c><00:00:27.279><c> on</c>

00:00:27.509 --> 00:00:27.519 align:start position:0%
components well with Kubernetes on
 

00:00:27.519 --> 00:00:29.029 align:start position:0%
components well with Kubernetes on
complex<00:00:27.840><c> hardware.</c><00:00:28.320><c> So</c><00:00:28.480><c> today</c><00:00:28.720><c> we</c><00:00:28.960><c> are</c>

00:00:29.029 --> 00:00:29.039 align:start position:0%
complex hardware. So today we are
 

00:00:29.039 --> 00:00:31.589 align:start position:0%
complex hardware. So today we are
introducing<00:00:29.519><c> a</c><00:00:29.840><c> simple</c><00:00:30.080><c> API</c><00:00:30.560><c> called</c><00:00:30.800><c> Grove</c>

00:00:31.589 --> 00:00:31.599 align:start position:0%
introducing a simple API called Grove
 

00:00:31.599 --> 00:00:33.750 align:start position:0%
introducing a simple API called Grove
which<00:00:31.840><c> allows</c><00:00:32.160><c> you</c><00:00:32.320><c> to</c><00:00:32.640><c> streamline</c><00:00:33.280><c> and</c><00:00:33.520><c> it</c>

00:00:33.750 --> 00:00:33.760 align:start position:0%
which allows you to streamline and it
 

00:00:33.760 --> 00:00:35.910 align:start position:0%
which allows you to streamline and it
makes<00:00:33.920><c> it</c><00:00:34.160><c> simple</c><00:00:34.399><c> to</c><00:00:34.800><c> deploy</c><00:00:35.200><c> this</c><00:00:35.600><c> very</c>

00:00:35.910 --> 00:00:35.920 align:start position:0%
makes it simple to deploy this very
 

00:00:35.920 --> 00:00:39.430 align:start position:0%
makes it simple to deploy this very
easily<00:00:36.239><c> on</c><00:00:36.559><c> Kubernetes</c><00:00:37.760><c> and</c><00:00:38.320><c> uh</c><00:00:38.559><c> manage</c><00:00:39.120><c> and</c>

00:00:39.430 --> 00:00:39.440 align:start position:0%
easily on Kubernetes and uh manage and
 

00:00:39.440 --> 00:00:40.869 align:start position:0%
easily on Kubernetes and uh manage and
abstract<00:00:39.840><c> away</c><00:00:40.079><c> all</c><00:00:40.239><c> the</c><00:00:40.399><c> complex</c>

00:00:40.869 --> 00:00:40.879 align:start position:0%
abstract away all the complex
 

00:00:40.879 --> 00:00:42.630 align:start position:0%
abstract away all the complex
infrastructure<00:00:41.520><c> underneath.</c><00:00:42.160><c> I</c><00:00:42.320><c> would</c><00:00:42.480><c> love</c>

00:00:42.630 --> 00:00:42.640 align:start position:0%
infrastructure underneath. I would love
 

00:00:42.640 --> 00:00:44.549 align:start position:0%
infrastructure underneath. I would love
for<00:00:42.879><c> you</c><00:00:43.040><c> to</c><00:00:43.200><c> check</c><00:00:43.360><c> it</c><00:00:43.600><c> out.</c>

00:00:44.549 --> 00:00:44.559 align:start position:0%
for you to check it out.
 

00:00:44.559 --> 00:00:46.470 align:start position:0%
for you to check it out.
&gt;&gt; Now<00:00:44.719><c> I'm</c><00:00:44.960><c> here</c><00:00:45.120><c> with</c><00:00:45.280><c> Mike</c><00:00:45.520><c> from</c><00:00:45.680><c> Nvidia.</c>

00:00:46.470 --> 00:00:46.480 align:start position:0%
&gt;&gt; Now I'm here with Mike from Nvidia.
 

00:00:46.480 --> 00:00:48.310 align:start position:0%
&gt;&gt; Now I'm here with Mike from Nvidia.
Mike,<00:00:47.120><c> can</c><00:00:47.280><c> you</c><00:00:47.360><c> help</c><00:00:47.520><c> me</c><00:00:47.680><c> better</c><00:00:47.920><c> understand</c>

00:00:48.310 --> 00:00:48.320 align:start position:0%
Mike, can you help me better understand
 

00:00:48.320 --> 00:00:50.389 align:start position:0%
Mike, can you help me better understand
my<00:00:48.640><c> performance</c><00:00:48.960><c> of</c><00:00:49.120><c> my</c><00:00:49.280><c> apps</c><00:00:49.520><c> on</c><00:00:49.680><c> the</c><00:00:49.840><c> cloud?</c>

00:00:50.389 --> 00:00:50.399 align:start position:0%
my performance of my apps on the cloud?
 

00:00:50.399 --> 00:00:51.830 align:start position:0%
my performance of my apps on the cloud?
Yeah,<00:00:50.559><c> it</c><00:00:50.800><c> can</c><00:00:50.879><c> be</c><00:00:51.039><c> really</c><00:00:51.280><c> challenging</c><00:00:51.600><c> to</c>

00:00:51.830 --> 00:00:51.840 align:start position:0%
Yeah, it can be really challenging to
 

00:00:51.840 --> 00:00:53.350 align:start position:0%
Yeah, it can be really challenging to
understand<00:00:52.559><c> uh</c><00:00:52.640><c> your</c><00:00:52.879><c> application</c>

00:00:53.350 --> 00:00:53.360 align:start position:0%
understand uh your application
 

00:00:53.360 --> 00:00:54.549 align:start position:0%
understand uh your application
performance<00:00:53.680><c> in</c><00:00:53.840><c> the</c><00:00:54.000><c> cloud,</c><00:00:54.239><c> especially</c>

00:00:54.549 --> 00:00:54.559 align:start position:0%
performance in the cloud, especially
 

00:00:54.559 --> 00:00:56.150 align:start position:0%
performance in the cloud, especially
when<00:00:54.719><c> you're</c><00:00:54.879><c> dealing</c><00:00:55.120><c> with</c><00:00:55.520><c> distributed</c>

00:00:56.150 --> 00:00:56.160 align:start position:0%
when you're dealing with distributed
 

00:00:56.160 --> 00:00:57.910 align:start position:0%
when you're dealing with distributed
applications<00:00:56.640><c> over</c><00:00:56.879><c> multiple</c><00:00:57.199><c> GPUs</c><00:00:57.680><c> over</c>

00:00:57.910 --> 00:00:57.920 align:start position:0%
applications over multiple GPUs over
 

00:00:57.920 --> 00:00:59.029 align:start position:0%
applications over multiple GPUs over
multiple<00:00:58.239><c> systems.</c><00:00:58.295><c> [music]</c><00:00:58.640><c> So,</c><00:00:58.800><c> you</c><00:00:58.960><c> can</c>

00:00:59.029 --> 00:00:59.039 align:start position:0%
multiple systems. [music] So, you can
 

00:00:59.039 --> 00:01:00.950 align:start position:0%
multiple systems. [music] So, you can
use<00:00:59.199><c> Insight</c><00:00:59.600><c> Cloud</c><00:00:59.840><c> to</c><00:01:00.079><c> be</c><00:01:00.160><c> able</c><00:01:00.320><c> to</c><00:01:00.559><c> profile</c>

00:01:00.950 --> 00:01:00.960 align:start position:0%
use Insight Cloud to be able to profile
 

00:01:00.960 --> 00:01:02.790 align:start position:0%
use Insight Cloud to be able to profile
your<00:01:01.120><c> workloads</c><00:01:01.600><c> and</c><00:01:02.079><c> uh</c><00:01:02.239><c> be</c><00:01:02.399><c> able</c><00:01:02.480><c> to</c><00:01:02.640><c> drill</c>

00:01:02.790 --> 00:01:02.800 align:start position:0%
your workloads and uh be able to drill
 

00:01:02.800 --> 00:01:05.030 align:start position:0%
your workloads and uh be able to drill
in,<00:01:03.039><c> find</c><00:01:03.120><c> out</c><00:01:03.280><c> where</c><00:01:03.440><c> your</c><00:01:03.520><c> bottlenecks</c><00:01:04.000><c> are.</c>

00:01:05.030 --> 00:01:05.040 align:start position:0%
in, find out where your bottlenecks are.
 

00:01:05.040 --> 00:01:07.190 align:start position:0%
in, find out where your bottlenecks are.
&gt;&gt; Now,<00:01:05.199><c> I'm</c><00:01:05.360><c> here</c><00:01:05.519><c> with</c><00:01:05.680><c> Egan</c><00:01:06.080><c> from</c><00:01:06.320><c> Nvidia.</c>

00:01:07.190 --> 00:01:07.200 align:start position:0%
&gt;&gt; Now, I'm here with Egan from Nvidia.
 

00:01:07.200 --> 00:01:08.710 align:start position:0%
&gt;&gt; Now, I'm here with Egan from Nvidia.
When<00:01:07.439><c> my</c><00:01:07.600><c> apps</c><00:01:07.840><c> are</c><00:01:08.000><c> getting</c><00:01:08.240><c> like</c><00:01:08.479><c> high</c>

00:01:08.710 --> 00:01:08.720 align:start position:0%
When my apps are getting like high
 

00:01:08.720 --> 00:01:10.630 align:start position:0%
When my apps are getting like high
traffic<00:01:09.040><c> and</c><00:01:09.280><c> need</c><00:01:09.439><c> to</c><00:01:09.600><c> scale</c><00:01:09.840><c> up,</c><00:01:10.159><c> my</c><00:01:10.320><c> pods</c>

00:01:10.630 --> 00:01:10.640 align:start position:0%
traffic and need to scale up, my pods
 

00:01:10.640 --> 00:01:12.469 align:start position:0%
traffic and need to scale up, my pods
are<00:01:10.799><c> just</c><00:01:10.960><c> sitting</c><00:01:11.119><c> there</c><00:01:11.360><c> doing</c><00:01:11.680><c> nothing.</c>

00:01:12.469 --> 00:01:12.479 align:start position:0%
are just sitting there doing nothing.
 

00:01:12.479 --> 00:01:14.070 align:start position:0%
are just sitting there doing nothing.
Can<00:01:12.720><c> you</c><00:01:12.799><c> tell</c><00:01:12.960><c> me</c><00:01:13.040><c> what's</c><00:01:13.280><c> going</c><00:01:13.439><c> on,</c><00:01:13.680><c> please?</c>

00:01:14.070 --> 00:01:14.080 align:start position:0%
Can you tell me what's going on, please?
 

00:01:14.080 --> 00:01:16.310 align:start position:0%
Can you tell me what's going on, please?
The<00:01:14.320><c> LLMs</c><00:01:14.880><c> are</c><00:01:15.040><c> getting</c><00:01:15.280><c> bigger</c><00:01:15.600><c> and</c><00:01:15.760><c> bigger</c>

00:01:16.310 --> 00:01:16.320 align:start position:0%
The LLMs are getting bigger and bigger
 

00:01:16.320 --> 00:01:18.550 align:start position:0%
The LLMs are getting bigger and bigger
every<00:01:16.640><c> day.</c><00:01:17.200><c> You</c><00:01:17.439><c> need</c><00:01:17.600><c> to</c><00:01:17.759><c> load</c><00:01:18.000><c> them</c><00:01:18.240><c> quickly</c>

00:01:18.550 --> 00:01:18.560 align:start position:0%
every day. You need to load them quickly
 

00:01:18.560 --> 00:01:20.789 align:start position:0%
every day. You need to load them quickly
when<00:01:18.799><c> your</c><00:01:19.040><c> users</c><00:01:19.439><c> are</c><00:01:19.680><c> waiting</c><00:01:20.240><c> to</c><00:01:20.479><c> get</c><00:01:20.560><c> an</c>

00:01:20.789 --> 00:01:20.799 align:start position:0%
when your users are waiting to get an
 

00:01:20.799 --> 00:01:23.990 align:start position:0%
when your users are waiting to get an
answer.<00:01:21.439><c> You</c><00:01:21.759><c> first</c><00:01:22.000><c> need</c><00:01:22.240><c> to</c><00:01:22.799><c> load</c><00:01:23.280><c> tensors</c>

00:01:23.990 --> 00:01:24.000 align:start position:0%
answer. You first need to load tensors
 

00:01:24.000 --> 00:01:26.950 align:start position:0%
answer. You first need to load tensors
from<00:01:24.560><c> storage</c><00:01:24.960><c> to</c><00:01:25.280><c> CPU</c><00:01:25.680><c> and</c><00:01:25.920><c> then</c><00:01:26.080><c> to</c><00:01:26.320><c> GPU</c>

00:01:26.950 --> 00:01:26.960 align:start position:0%
from storage to CPU and then to GPU
 

00:01:26.960 --> 00:01:28.469 align:start position:0%
from storage to CPU and then to GPU
takes<00:01:27.200><c> a</c><00:01:27.439><c> lot</c><00:01:27.520><c> of</c><00:01:27.680><c> time</c><00:01:27.840><c> and</c><00:01:28.080><c> it's</c><00:01:28.320><c> not</c>

00:01:28.469 --> 00:01:28.479 align:start position:0%
takes a lot of time and it's not
 

00:01:28.479 --> 00:01:30.390 align:start position:0%
takes a lot of time and it's not
optimized<00:01:29.040><c> well</c><00:01:29.280><c> enough.</c><00:01:29.600><c> So</c><00:01:29.840><c> in</c><00:01:30.080><c> order</c><00:01:30.240><c> to</c>

00:01:30.390 --> 00:01:30.400 align:start position:0%
optimized well enough. So in order to
 

00:01:30.400 --> 00:01:32.230 align:start position:0%
optimized well enough. So in order to
solve<00:01:30.720><c> that,</c><00:01:31.119><c> it's</c><00:01:31.360><c> called</c><00:01:31.520><c> RAI</c><00:01:31.920><c> model</c>

00:01:32.230 --> 00:01:32.240 align:start position:0%
solve that, it's called RAI model
 

00:01:32.240 --> 00:01:33.749 align:start position:0%
solve that, it's called RAI model
streamer.<00:01:32.560><c> We</c><00:01:32.720><c> are</c><00:01:32.799><c> working</c><00:01:32.960><c> with</c><00:01:33.119><c> the</c><00:01:33.200><c> GKE</c>

00:01:33.749 --> 00:01:33.759 align:start position:0%
streamer. We are working with the GKE
 

00:01:33.759 --> 00:01:36.310 align:start position:0%
streamer. We are working with the GKE
team.<00:01:34.240><c> So</c><00:01:34.479><c> it</c><00:01:34.960><c> supports</c><00:01:35.360><c> cloud</c><00:01:35.759><c> storage</c><00:01:36.000><c> on</c>

00:01:36.310 --> 00:01:36.320 align:start position:0%
team. So it supports cloud storage on
 

00:01:36.320 --> 00:01:38.789 align:start position:0%
team. So it supports cloud storage on
any<00:01:36.560><c> storage</c><00:01:36.880><c> types.</c><00:01:37.600><c> We</c><00:01:37.920><c> stream</c><00:01:38.479><c> model</c>

00:01:38.789 --> 00:01:38.799 align:start position:0%
any storage types. We stream model
 

00:01:38.799 --> 00:01:41.190 align:start position:0%
any storage types. We stream model
weights<00:01:39.600><c> uh</c><00:01:39.680><c> to</c><00:01:39.840><c> GPU</c><00:01:40.320><c> while</c><00:01:40.560><c> reading</c><00:01:40.880><c> them</c>

00:01:41.190 --> 00:01:41.200 align:start position:0%
weights uh to GPU while reading them
 

00:01:41.200 --> 00:01:43.429 align:start position:0%
weights uh to GPU while reading them
concurrently<00:01:41.840><c> from</c><00:01:42.320><c> any</c><00:01:42.640><c> storage</c><00:01:42.960><c> type.</c><00:01:43.360><c> We</c>

00:01:43.429 --> 00:01:43.439 align:start position:0%
concurrently from any storage type. We
 

00:01:43.439 --> 00:01:45.030 align:start position:0%
concurrently from any storage type. We
are<00:01:43.600><c> saving</c><00:01:43.920><c> a</c><00:01:44.079><c> lot</c><00:01:44.159><c> of</c><00:01:44.320><c> times.</c><00:01:44.720><c> We</c><00:01:44.880><c> have</c>

00:01:45.030 --> 00:01:45.040 align:start position:0%
are saving a lot of times. We have
 

00:01:45.040 --> 00:01:46.870 align:start position:0%
are saving a lot of times. We have
amazing<00:01:45.439><c> benchmarks</c><00:01:46.000><c> that</c><00:01:46.159><c> are</c><00:01:46.399><c> coming</c><00:01:46.560><c> up.</c>

00:01:46.870 --> 00:01:46.880 align:start position:0%
amazing benchmarks that are coming up.
 

00:01:46.880 --> 00:01:48.230 align:start position:0%
amazing benchmarks that are coming up.
You<00:01:47.119><c> should</c><00:01:47.280><c> give</c><00:01:47.360><c> it</c><00:01:47.520><c> a</c><00:01:47.680><c> try.</c>

00:01:48.230 --> 00:01:48.240 align:start position:0%
You should give it a try.
 

00:01:48.240 --> 00:01:50.230 align:start position:0%
You should give it a try.
&gt;&gt; I<00:01:48.479><c> will.</c><00:01:48.799><c> Thanks,</c><00:01:49.040><c> Egan.</c><00:01:49.520><c> And</c><00:01:49.680><c> you</c><00:01:49.920><c> can</c><00:01:50.000><c> find</c>

00:01:50.230 --> 00:01:50.240 align:start position:0%
&gt;&gt; I will. Thanks, Egan. And you can find
 

00:01:50.240 --> 00:01:52.069 align:start position:0%
&gt;&gt; I will. Thanks, Egan. And you can find
this<00:01:50.479><c> and</c><00:01:50.720><c> more</c><00:01:50.960><c> at</c><00:01:51.200><c> the</c><00:01:51.360><c> Google</c><00:01:51.600><c> Cloud</c><00:01:51.920><c> and</c>

00:01:52.069 --> 00:01:52.079 align:start position:0%
this and more at the Google Cloud and
 

00:01:52.079 --> 00:01:57.399 align:start position:0%
this and more at the Google Cloud and
Nvidia<00:01:52.720><c> developer</c><00:01:53.280><c> community.</c><00:01:54.079><c> Join</c><00:01:54.399><c> now.</c>

